{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import glob\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from standardization import calculate_standardization_from_files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "files = glob.glob(\"/scratch/users/schuetzn/data/mhc_dataset/*/metadata.parquet\")\n",
    "standardization_df = calculate_standardization_from_files(files)\n",
    "standardization_df.to_csv(\"/scratch/users/schuetzn/data/mhc_dataset_out/standardization_params.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization_df = pd.read_csv(\"/scratch/users/schuetzn/data/mhc_dataset_out/standardization_params.csv\")\n",
    "standardization_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "dfs = []\n",
    "files = glob.glob(\"/scratch/users/schuetzn/data/mhc_dataset/*/metadata.parquet\")\n",
    "for file in files:\n",
    "    hq = file.split(\"/\")[-2]\n",
    "    df = pd.read_parquet(file)\n",
    "    df[\"healthCode\"] = hq\n",
    "    dfs.append(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.concat(dfs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stats_per_modality = []\n",
    "for idx, row in df.groupby([\"healthCode\", \"date\"]):\n",
    "    # Common data for all entries\n",
    "    healthCode = row[\"healthCode\"].iloc[0]\n",
    "    date = row[\"date\"].iloc[0]\n",
    "    \n",
    "    # Map specific variables to their corresponding indices in the dataframe\n",
    "    variable_mappings = {\n",
    "        \"WorkoutsAny\": [14, 15, 16, 17, 18, 19, 20, 21, 22, 23],  # All workout types\n",
    "        \"WorkoutsRunning\": [16],  # HKWorkoutActivityTypeRunning\n",
    "        \"WorkoutsCycling\": [15],  # HKWorkoutActivityTypeCycling\n",
    "        \"HeartRate\": [5],  # HKQuantityTypeIdentifierHeartRate\n",
    "        \"StepsTaken\": [0],  # HKQuantityTypeIdentifierStepCount\n",
    "        \"DistanceWalkingRunning\": [2],  # HKQuantityTypeIdentifierDistanceWalkingRunning\n",
    "        \"DistanceCycling\": [3],  # HKQuantityTypeIdentifierDistanceCycling\n",
    "        \"ActiveCaloriesBurned\": [1],  # HKQuantityTypeIdentifierActiveEnergyBurned\n",
    "        \"SleepAny\": [12, 13]  # Both sleep types - Asleep and InBed\n",
    "    }\n",
    "    \n",
    "    # Create entry for this healthCode/date combination\n",
    "    entry = {\n",
    "        \"healthCode\": healthCode,\n",
    "        \"date\": date\n",
    "    }\n",
    "    \n",
    "    # Calculate data_coverage_sum for each requested variable\n",
    "    for variable_name, indices in variable_mappings.items():\n",
    "        # For variables that map to multiple indices, sum the data_coverage across those indices\n",
    "        if len(indices) > 1:\n",
    "            coverage_sum = row.iloc[indices].data_coverage.sum()\n",
    "        else:\n",
    "            coverage_sum = row.iloc[indices[0]].data_coverage.sum()\n",
    "        \n",
    "        entry[f\"{variable_name}_coverage_sum\"] = coverage_sum\n",
    "    \n",
    "    daily_stats_per_modality.append(entry)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(daily_stats_per_modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "#with open(\"../per_modality_daily_v2.pkl\", \"wb\") as f:\n",
    "#   pickle.dump(daily_stats_per_modality, f)\n",
    "with open(\"../per_modality_daily_v2.pkl\", \"rb\") as f:\n",
    "     daily_stats_per_modality = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_by_modality_df = pd.DataFrame(daily_stats_per_modality)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_by_modality_df\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a dictionary to store the results\n",
    "modality_stats = {}\n",
    "\n",
    "# Get the list of modalities (excluding healthCode and date columns)\n",
    "modalities = [col for col in coverage_by_modality_df.columns if col not in ['healthCode', 'date']]\n",
    "\n",
    "# For each modality, calculate days > 0, unique participants, and estimated data points\n",
    "for modality in modalities:\n",
    "    # Filter to only rows where coverage > 0 for this modality\n",
    "    covered_data = coverage_by_modality_df[coverage_by_modality_df[modality] > 0]\n",
    "    \n",
    "    # Count unique days and participants\n",
    "    days_count = len(covered_data)\n",
    "    participants_count = covered_data['healthCode'].nunique()\n",
    "    \n",
    "    # Calculate estimated total data points (coverage of 100 = 1440 data points)\n",
    "    total_coverage = coverage_by_modality_df[modality].sum()\n",
    "    estimated_data_points = total_coverage * (1440/100)\n",
    "    \n",
    "    # Store in results dictionary\n",
    "    modality_stats[modality] = {\n",
    "        'days_with_coverage': days_count,\n",
    "        'unique_participants': participants_count,\n",
    "        'estimated_data_points': int(estimated_data_points)  # Convert to integer for cleaner display\n",
    "    }\n",
    "\n",
    "# Convert to a DataFrame for better display\n",
    "result_df = pd.DataFrame.from_dict(modality_stats, orient='index')\n",
    "\n",
    "# Rename the modalities for clarity\n",
    "modality_names = {\n",
    "    'WorkoutsAny_coverage_sum': 'Workouts (Any Type)',\n",
    "    'WorkoutsRunning_coverage_sum': 'Workouts - Running',\n",
    "    'WorkoutsCycling_coverage_sum': 'Workouts - Cycling',\n",
    "    'HeartRate_coverage_sum': 'Heart Rate',\n",
    "    'StepsTaken_coverage_sum': 'Steps Taken',\n",
    "    'DistanceWalkingRunning_coverage_sum': 'Distance Walking/Running',\n",
    "    'DistanceCycling_coverage_sum': 'Distance Cycling',\n",
    "    'ActiveCaloriesBurned_coverage_sum': 'Active Calories Burned',\n",
    "    'SleepAny_coverage_sum': 'Sleep (Any Type)'\n",
    "}\n",
    "\n",
    "# Create a new dataframe with clear modality names\n",
    "clear_result_df = result_df.copy()\n",
    "clear_result_df.index = [modality_names[idx] for idx in clear_result_df.index]\n",
    "\n",
    "# Format the estimated data points with commas for readability\n",
    "clear_result_df['estimated_data_points'] = clear_result_df['estimated_data_points'].apply(lambda x: f\"{x:,}\")\n",
    "\n",
    "# Display the results\n",
    "print(\"Coverage statistics for each modality:\")\n",
    "print(clear_result_df)\n",
    "\n",
    "# Optionally, sort by number of days with coverage to see most common modalities\n",
    "print(\"\\nModalities sorted by coverage (most to least):\")\n",
    "print(clear_result_df.sort_values('days_with_coverage', ascending=False))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "stats = df.groupby(\"healthCode\")[\"date\"].apply(lambda x: len(np.unique(x)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "counts_series = coverage_df.groupby(\"healthCode\").data_coverage_sum.count()\n",
    "thresholds = [1, 3, 7, 14, 30, 365, 5 * 365]\n",
    "values = [(counts_series > t).sum() for t in thresholds]\n",
    "counts = [(counts_series > t).sum() for t in thresholds]\n",
    "\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=[str(t) for t in thresholds], y=counts, palette=\"viridis\")\n",
    "for i, count in enumerate(counts):\n",
    "    ax.text(i, count + max(counts) * 0.01, str(count), ha='center', va='bottom', fontsize=12)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Number of rows with count > threshold\")\n",
    "plt.title(\"Rows with counts above various thresholds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stats = []\n",
    "for idx, row in df.groupby([\"healthCode\", \"date\"]):\n",
    "    data_coverage_sum = row.data_coverage.sum()\n",
    "    data_coverage_mean = row.data_coverage.mean()\n",
    "    n_above_zero = row[row.data_coverage > 0]\n",
    "    daily_stats.append({\n",
    "        \"data_coverage_sum\": data_coverage_sum,\n",
    "        \"data_coverage_mean\": data_coverage_mean,\n",
    "        \"n_above_zero\": len(n_above_zero),\n",
    "        \"healthCode\": row[\"healthCode\"].iloc[0]\n",
    "    })"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_coverage_mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(daily_stats)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "daily_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "# with open(\"../summary_daily_v2.pkl\", \"wb\") as f:\n",
    "#     pickle.dump(daily_stats, f)\n",
    "with open(\"../summary_daily_v2.pkl\", \"rb\") as f:\n",
    "   daily_stats = pickle.load(f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.groupby(\"healthCode\").data_coverage_sum.count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "info_df = pd.read_parquet('../myheart-counts-client/mhc_full_participant_info.parquet')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "healthCodes_open_sharing = set(info_df[info_df.sharingScope == \"all_qualified_researchers\"].healthCode)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df = pd.DataFrame(daily_stats)\n",
    "len(coverage_df), coverage_df.data_coverage_sum.quantile(0.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df[(coverage_df.data_coverage_sum > coverage_df.data_coverage_sum.quantile(0.25)) & (coverage_df.n_above_zero > 4)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df = coverage_df[coverage_df.healthCode.isin(healthCodes_open_sharing)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df.data_coverage_sum.quantile(0.25)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Days with ANY data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "len(coverage_df[coverage_df.data_coverage_sum > 0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "coverage_df[coverage_df.data_coverage_sum > 0].healthCode.unique().shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import numpy as np\n",
    "\n",
    "thresholds = [2, 3, 5, 8, 10, 15, 24]\n",
    "counts = [len(coverage_df[coverage_df.n_above_zero >= t]) for t in thresholds]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=[str(t) for t in thresholds], y=counts, palette=\"viridis\")\n",
    "for i, count in enumerate(counts):\n",
    "    ax.text(i, count + max(counts)*0.01, str(count), ha='center', va='bottom', fontsize=12)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Number of rows with at least k number of modalities\")\n",
    "#plt.title(\"Rows with n_above_zero above various thresholds\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "\n",
    "thresholds = [2, 3, 5, 8, 10, 15, 24]\n",
    "unique_counts = [coverage_df[coverage_df.n_above_zero > t]['healthCode'].nunique() for t in thresholds]\n",
    "\n",
    "sns.set(style=\"whitegrid\")\n",
    "plt.figure(figsize=(10, 6))\n",
    "ax = sns.barplot(x=[str(t) for t in thresholds], y=unique_counts, palette=\"viridis\")\n",
    "for i, count in enumerate(unique_counts):\n",
    "    ax.text(i, count + max(unique_counts)*0.01, str(count), ha='center', va='bottom', fontsize=12)\n",
    "plt.xlabel(\"Threshold\")\n",
    "plt.ylabel(\"Unique healthCode count\")\n",
    "plt.title(\"Unique healthCodes for rows with n_above_zero > threshold\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "from typing import List \n",
    "from tqdm.auto import tqdm # Optional: for progress bar\n",
    "\n",
    "def calculate_standardization_from_files(metadata_filepaths: List[str]) -> pd.DataFrame:\n",
    "    \"\"\"\n",
    "    Calculates the overall mean and standard deviation for each feature \n",
    "    by aggregating statistics from a provided list of metadata files.\n",
    "\n",
    "    The metadata files are expected to have been created by the \n",
    "    `create_dataset` function (or similar), containing columns 'n', 'sum', \n",
    "    'sum_of_squares', and an index representing the feature identifier \n",
    "    (expected to be named 'feature_index').\n",
    "\n",
    "    Args:\n",
    "        metadata_filepaths: A list of strings, where each string is the full \n",
    "                             path to a metadata file (e.g., a .parquet file).\n",
    "\n",
    "    Returns:\n",
    "        A pandas DataFrame indexed by 'feature_index' with columns 'mean' \n",
    "        and 'std_dev' representing the calculated standardization parameters \n",
    "        across all provided files. Returns an empty DataFrame if the list \n",
    "        is empty, no files could be processed, or they lack the required \n",
    "        columns/index.\n",
    "    \"\"\"\n",
    "    if not metadata_filepaths:\n",
    "        print(\"Warning: Received an empty list of metadata file paths.\")\n",
    "        return pd.DataFrame(columns=['mean', 'std_dev'])\n",
    "\n",
    "    # Use defaultdict to easily accumulate sums per feature index\n",
    "    aggregated_stats = defaultdict(lambda: {'total_n': 0.0, 'total_sum': 0.0, 'total_sum_of_squares': 0.0})\n",
    "\n",
    "    print(f\"Processing {len(metadata_filepaths)} metadata files. Aggregating statistics...\")\n",
    "    # Use tqdm if installed for a progress bar, otherwise just iterate\n",
    "    file_iterator = tqdm(metadata_filepaths) if 'tqdm' in globals() else metadata_filepaths\n",
    "    \n",
    "    for file_path in file_iterator:\n",
    "        try:\n",
    "            df = pd.read_parquet(file_path)\n",
    "\n",
    "            # Verify necessary columns and index name\n",
    "            required_cols = {'n', 'sum', 'sum_of_squares'}\n",
    "            if not required_cols.issubset(df.columns):\n",
    "                print(f\"Warning: Skipping {file_path}. Missing required columns: {required_cols - set(df.columns)}\")\n",
    "                continue\n",
    "                \n",
    "            # Check for 'feature_index' either as index name or column\n",
    "            if df.index.name != 'feature_index':\n",
    "                 if 'feature_index' in df.columns:\n",
    "                     # Promote 'feature_index' column to be the index\n",
    "                     df = df.set_index('feature_index')\n",
    "                 else:\n",
    "                    # Try using the existing index if it's unnamed, hoping it's the feature index\n",
    "                    if df.index.name is None:\n",
    "                        print(f\"Warning: Index name in {file_path} is not 'feature_index'. Assuming the unnamed index represents features.\")\n",
    "                        df.index.name = 'feature_index' # Assign the expected name\n",
    "                    else:\n",
    "                        print(f\"Warning: Skipping {file_path}. Index name is '{df.index.name}' (expected 'feature_index') and column not found.\")\n",
    "                        continue\n",
    "\n",
    "            # Ensure data types are appropriate for summation\n",
    "            try:\n",
    "                df[list(required_cols)] = df[list(required_cols)].astype(float)\n",
    "            except ValueError as e:\n",
    "                 print(f\"Warning: Skipping {file_path}. Could not convert required columns to float: {e}\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "            # Iterate through features in the current file and add to totals\n",
    "            for feature_idx, row in df.iterrows():\n",
    "                 # Check for NaN values in stats before aggregating\n",
    "                 if pd.isna(row['n']) or pd.isna(row['sum']) or pd.isna(row['sum_of_squares']):\n",
    "                     # Optionally print a warning about NaN stats, or just skip\n",
    "                     # print(f\"Warning: Found NaN statistics for feature {feature_idx} in {file_path}. Skipping this row.\")\n",
    "                     continue\n",
    "                 \n",
    "                 aggregated_stats[feature_idx]['total_n'] += row['n']\n",
    "                 aggregated_stats[feature_idx]['total_sum'] += row['sum']\n",
    "                 aggregated_stats[feature_idx]['total_sum_of_squares'] += row['sum_of_squares']\n",
    "\n",
    "        except FileNotFoundError:\n",
    "            print(f\"Error: File not found {file_path}. Skipping.\")\n",
    "            continue\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing {file_path}: {e}. Skipping.\")\n",
    "            continue # Skip to the next file on error\n",
    "\n",
    "    print(\"Aggregation complete. Calculating final standardization parameters...\")\n",
    "    \n",
    "    results = []\n",
    "    for feature_idx, stats in aggregated_stats.items():\n",
    "        total_n = stats['total_n']\n",
    "        total_sum = stats['total_sum']\n",
    "        total_sum_of_squares = stats['total_sum_of_squares']\n",
    "\n",
    "        if total_n > 0:\n",
    "            mean = total_sum / total_n\n",
    "            # Ensure variance is non-negative due to potential floating point issues\n",
    "            # Also handle cases where sum_of_squares might be slightly less than sum^2/n due to precision\n",
    "            variance_raw = (total_sum_of_squares / total_n) - (mean ** 2)\n",
    "            variance = max(0, variance_raw) \n",
    "            std_dev = np.sqrt(variance)\n",
    "            \n",
    "            # Optional: Add a check for extremely small variance close to zero\n",
    "            # if variance < 1e-10: # Adjust tolerance as needed\n",
    "            #     std_dev = 0.0 \n",
    "                \n",
    "        else:\n",
    "            # Handle cases with no valid data points for a feature\n",
    "            mean = np.nan \n",
    "            std_dev = np.nan\n",
    "            print(f\"Warning: Feature index {feature_idx} had total_n = 0 across all processed files.\")\n",
    "\n",
    "        results.append({'feature_index': feature_idx, 'mean': mean, 'std_dev': std_dev})\n",
    "\n",
    "    if not results:\n",
    "        print(\"No statistics were aggregated successfully from the provided files.\")\n",
    "        return pd.DataFrame(columns=['mean', 'std_dev'])\n",
    "\n",
    "    # Create final DataFrame\n",
    "    final_params_df = pd.DataFrame(results)\n",
    "    final_params_df = final_params_df.set_index('feature_index')\n",
    "    final_params_df = final_params_df.sort_index() # Sort by feature index for consistency\n",
    "\n",
    "    print(\"Standardization parameters calculated.\")\n",
    "    return final_params_df\n",
    "\n",
    "files = glob.glob(\"/scratch/users/schuetzn/data/mhc_dataset/*/metadata.parquet\")\n",
    "stat_df = calculate_standardization_from_files(files)\n",
    "stat_df.to_parquet(\"standardization_params.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "df = pd.read_parquet(\"/home/users/schuetzn/MHC_Dataset/standardization_params.parquet\")\n",
    "df"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
