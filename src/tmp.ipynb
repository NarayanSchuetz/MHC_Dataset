{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "interval_df = pd.read_csv(\"~/Downloads/sherlock/valid_7day_windows.csv\")\n",
    "interval_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import re # Import regular expressions module\n",
    "\n",
    "\n",
    "# Updated function to handle standard lists and space-separated number lists\n",
    "def parse_string_list_safely(item):\n",
    "    if pd.isna(item):\n",
    "        return np.nan\n",
    "    elif isinstance(item, list):\n",
    "         return item # Already a list\n",
    "    elif isinstance(item, str):\n",
    "        original_item_repr = repr(item) # Get a representation for logging\n",
    "        item = item.strip() # Remove leading/trailing whitespace\n",
    "        if not item:\n",
    "            return np.nan # Handle empty strings\n",
    "\n",
    "        # Attempt 1: Standard Python list literal parsing\n",
    "        try:\n",
    "            evaluated = ast.literal_eval(item)\n",
    "            if isinstance(evaluated, list):\n",
    "                return evaluated\n",
    "            else:\n",
    "                # If it evaluates to a single item, wrap it in a list?\n",
    "                # Example: \"'[10.]'\" might eval to 10.0 if quotes were weird.\n",
    "                # Let's assume we always want list output from strings like '[...]'\n",
    "                # If the original string looks like a list, but evaluates to single item:\n",
    "                if item.startswith('[') and item.endswith(']'):\n",
    "                     return [evaluated] # Wrap single valid item in list\n",
    "                else:\n",
    "                     # Or treat non-list results from valid literals as error?\n",
    "                     print(f\"Warning: Evaluated item {original_item_repr} is not a list, treating as NaN.\")\n",
    "                     return np.nan\n",
    "        except (ValueError, SyntaxError, TypeError):\n",
    "            # If standard parsing fails, proceed to Attempt 2\n",
    "            pass # Fall through to the next attempt\n",
    "\n",
    "        # Attempt 2: Space-separated numbers within brackets\n",
    "        if item.startswith('[') and item.endswith(']'):\n",
    "            content = item[1:-1].strip() # Extract content within brackets\n",
    "            if not content: # Handle empty brackets '[]'\n",
    "                return []\n",
    "            try:\n",
    "                # Split by whitespace, filter empty strings, convert to float\n",
    "                # Using regex \\s+ handles multiple spaces correctly\n",
    "                parsed_list = [float(num_str) for num_str in re.split(r'\\s+', content) if num_str]\n",
    "                return parsed_list\n",
    "            except ValueError:\n",
    "                # Handle cases where content isn't purely space-separated numbers\n",
    "                print(f\"Warning: Could not parse content of {original_item_repr} as space-separated numbers, treating as NaN.\")\n",
    "                return np.nan\n",
    "            except Exception as e:\n",
    "                 print(f\"Warning: Unexpected error parsing space-separated list {original_item_repr}: {e}, treating as NaN.\")\n",
    "                 return np.nan\n",
    "\n",
    "        # If neither parsing method worked\n",
    "        print(f\"Warning: Could not parse string {original_item_repr} as any known list format, treating as NaN.\")\n",
    "        return np.nan\n",
    "    else:\n",
    "        print(f\"Warning: Unexpected type {type(item)} for item '{item}', treating as NaN.\")\n",
    "        return np.nan"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "label_df = pd.read_csv(\"~/Downloads/sherlock/combined_mhc_data.csv\")\n",
    "label_df.createdOn = pd.to_datetime(label_df.createdOn, format='ISO8601')\n",
    "label_df.heart_disease = label_df.heart_disease.apply(parse_string_list_safely)\n",
    "label_df = label_df.explode('heart_disease')\n",
    "label_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 82,
   "metadata": {},
   "outputs": [],
   "source": [
    "labels = ['sleep_diagnosis1', 'happiness', 'heart_disease', 'feel_worthwhile1', \n",
    "          'feel_worthwhile2', 'feel_worthwhile3', 'Diabetes', 'Hypertension']\n",
    "\n",
    "for label in labels:\n",
    "    label_df[label] = label_df[label].apply(lambda x: np.nan if np.isnan(x) else int(x))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 121,
   "metadata": {},
   "outputs": [],
   "source": [
    "hcs = list(interval_df.healthCode.unique())\n",
    "\n",
    "global_records = []\n",
    "for hc in hcs:\n",
    "    i_df = interval_df[interval_df.healthCode == hc].copy()\n",
    "    if i_df.empty:\n",
    "        continue\n",
    "    \n",
    "    records = i_df.to_dict('records')\n",
    "\n",
    "    for label in labels:\n",
    "        l_df = label_df[(label_df.healthCode == hc) & (label_df[label].notna())].copy()\n",
    "        if l_df.empty:\n",
    "            continue\n",
    "        dates = l_df.createdOn.dt.date.astype(str)\n",
    "        matches = find_closest_dates(dates, i_df.time_range)\n",
    "        for record, match_key in zip(records, matches):\n",
    "            match_idx = matches[match_key]\n",
    "            label_dict = {\n",
    "                'label_value': l_df.iloc[match_idx][label],\n",
    "                'label_date': l_df.iloc[match_idx]['createdOn']\n",
    "            }\n",
    "            record[label] = label_dict\n",
    "    global_records.extend(records)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "from pathlib import Path\n",
    "\n",
    "# Convert numpy values to Python native types for JSON serialization\n",
    "def convert_numpy_to_native(obj):\n",
    "    if isinstance(obj, dict):\n",
    "        return {k: convert_numpy_to_native(v) for k, v in obj.items()}\n",
    "    elif isinstance(obj, list):\n",
    "        return [convert_numpy_to_native(item) for item in obj]\n",
    "    elif hasattr(obj, 'item'):  # Check if it's a numpy scalar\n",
    "        return obj.item()\n",
    "    else:\n",
    "        return obj\n",
    "\n",
    "# Convert records to JSON-serializable format\n",
    "serializable_records = convert_numpy_to_native(global_records)\n",
    "\n",
    "# Create output path\n",
    "output_path = os.path.expanduser(\"~/Downloads/global_records.json\")\n",
    "\n",
    "# Write to JSON file\n",
    "with open(output_path, 'w') as f:\n",
    "    json.dump(serializable_records, f, indent=2, default=str)\n",
    "\n",
    "# Display first record as confirmation\n",
    "global_records[0] if global_records else \"No records found\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create a denormalized pandas dataframe from global_records\n",
    "import pandas as pd\n",
    "\n",
    "# First, flatten nested dictionaries in global_records\n",
    "flattened_records = []\n",
    "for record in global_records:\n",
    "    flat_record = {}\n",
    "    for key, value in record.items():\n",
    "        if isinstance(value, dict) and 'label_value' in value:\n",
    "            # Flatten label dictionaries\n",
    "            flat_record[f\"{key}_value\"] = value['label_value']\n",
    "            flat_record[f\"{key}_date\"] = value['label_date']\n",
    "        else:\n",
    "            flat_record[key] = value\n",
    "    flattened_records.append(flat_record)\n",
    "\n",
    "# Create pandas DataFrame from flattened records\n",
    "denormalized_df = pd.DataFrame(flattened_records)\n",
    "\n",
    "# Display the first few rows\n",
    "denormalized_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "denormalized_df.to_parquet(\"~/Downloads/global_records.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Display value counts for each column in label_df\n",
    "for column in labels:\n",
    "    if column in ['healthCode', 'createdOn', 'file_uris']:\n",
    "        # Skip non-categorical columns\n",
    "        continue\n",
    "    print(label_df[column].value_counts())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datetime import datetime\n",
    "import bisect\n",
    "\n",
    "def find_closest_dates(dates_list, intervals_list):\n",
    "    \"\"\"\n",
    "    For each interval, find the closest date in the dates_list.\n",
    "    \n",
    "    Args:\n",
    "        dates_list: List of date strings in format 'YYYY-MM-DD'\n",
    "        intervals_list: List of interval strings in format 'YYYY-MM-DD_YYYY-MM-DD'\n",
    "        \n",
    "    Returns:\n",
    "        Dictionary with intervals as keys and indices of closest dates as values\n",
    "    \"\"\"\n",
    "    # Convert dates_list to datetime objects\n",
    "    dates_dt = [datetime.strptime(date, \"%Y-%m-%d\") for date in dates_list]\n",
    "    \n",
    "    # Sort dates for binary search\n",
    "    sorted_dates_with_indices = sorted(enumerate(dates_dt), key=lambda x: x[1])\n",
    "    sorted_dates = [d for _, d in sorted_dates_with_indices]\n",
    "    original_indices = [i for i, _ in sorted_dates_with_indices]\n",
    "    \n",
    "    result = {}\n",
    "    \n",
    "    for interval in intervals_list:\n",
    "        # Parse start and end dates\n",
    "        start_str, end_str = interval.split('_')\n",
    "        start_date = datetime.strptime(start_str, \"%Y-%m-%d\")\n",
    "        end_date = datetime.strptime(end_str, \"%Y-%m-%d\")\n",
    "        \n",
    "        # Calculate midpoint\n",
    "        midpoint = start_date + (end_date - start_date) / 2\n",
    "        \n",
    "        # Find the closest date using binary search\n",
    "        pos = bisect.bisect_left(sorted_dates, midpoint)\n",
    "        \n",
    "        # Handle edge cases\n",
    "        if pos == 0:\n",
    "            closest_idx = 0\n",
    "        elif pos == len(sorted_dates):\n",
    "            closest_idx = len(sorted_dates) - 1\n",
    "        else:\n",
    "            # Check which is closer\n",
    "            if abs(sorted_dates[pos] - midpoint) < abs(sorted_dates[pos-1] - midpoint):\n",
    "                closest_idx = pos\n",
    "            else:\n",
    "                closest_idx = pos - 1\n",
    "        \n",
    "        # Get original index\n",
    "        original_idx = original_indices[closest_idx]\n",
    "        result[interval] = original_idx\n",
    "    \n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "df0 = pd.read_parquet(\"~/Downloads/global_records.parquet\")\n",
    "df1 = pd.read_parquet(\"~/Downloads/global_records_1.parquet\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compare the two dataframes\n",
    "print(\"Shape comparison:\")\n",
    "print(f\"df0 shape: {df0.shape}\")\n",
    "print(f\"df1 shape: {df1.shape}\")\n",
    "\n",
    "# Check if they have the same columns\n",
    "print(\"\\nColumn comparison:\")\n",
    "df0_cols = set(df0.columns)\n",
    "df1_cols = set(df1.columns)\n",
    "print(f\"Columns only in df0: {df0_cols - df1_cols}\")\n",
    "print(f\"Columns only in df1: {df1_cols - df0_cols}\")\n",
    "print(f\"Common columns: {len(df0_cols.intersection(df1_cols))}\")\n",
    "\n",
    "# Compare values for common columns\n",
    "common_cols = list(df0_cols.intersection(df1_cols))\n",
    "print(\"\\nValue comparison for common columns:\")\n",
    "for col in common_cols[:5]:  # Limit to first 5 columns to avoid excessive output\n",
    "    if df0[col].equals(df1[col]):\n",
    "        print(f\"Column '{col}' has identical values\")\n",
    "    else:\n",
    "        print(f\"Column '{col}' has differences\")\n",
    "        # Show a sample of differences\n",
    "        mask = ~(df0[col] == df1[col])\n",
    "        if mask.any():\n",
    "            diff_sample = pd.DataFrame({\n",
    "                'df0': df0.loc[mask, col].head(3),\n",
    "                'df1': df1.loc[mask, col].head(3)\n",
    "            })\n",
    "            print(diff_sample)\n",
    "\n",
    "# Check for NaN differences\n",
    "print(\"\\nNaN value comparison:\")\n",
    "nan_diff_cols = []\n",
    "for col in common_cols:\n",
    "    df0_nulls = df0[col].isna().sum()\n",
    "    df1_nulls = df1[col].isna().sum()\n",
    "    if df0_nulls != df1_nulls:\n",
    "        nan_diff_cols.append((col, df0_nulls, df1_nulls))\n",
    "\n",
    "if nan_diff_cols:\n",
    "    print(\"Columns with different NaN counts:\")\n",
    "    for col, df0_nulls, df1_nulls in nan_diff_cols[:5]:  # Limit to first 5\n",
    "        print(f\"'{col}': df0={df0_nulls} nulls, df1={df1_nulls} nulls\")\n",
    "else:\n",
    "    print(\"All columns have the same number of NaN values\")\n",
    "\n",
    "# Check for duplicate rows\n",
    "print(\"\\nDuplicate row comparison:\")\n",
    "print(f\"df0 duplicated rows: {df0.duplicated().sum()}\")\n",
    "print(f\"df1 duplicated rows: {df1.duplicated().sum()}\")\n",
    "\n",
    "# Summary statistics comparison for numeric columns\n",
    "print(\"\\nSummary statistics comparison (first numeric column):\")\n",
    "numeric_cols = [col for col in common_cols if pd.api.types.is_numeric_dtype(df0[col])]\n",
    "if numeric_cols:\n",
    "    col = numeric_cols[0]\n",
    "    print(f\"Column: {col}\")\n",
    "    print(\"df0 stats:\")\n",
    "    print(df0[col].describe())\n",
    "    print(\"df1 stats:\")\n",
    "    print(df1[col].describe())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the global_records1.json file\n",
    "import json\n",
    "\n",
    "with open('/Users/narayanschuetz/Downloads/global_records_1.json', 'r') as f:\n",
    "    global_records = json.load(f)\n",
    "    \n",
    "# Display the loaded data\n",
    "global_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if 'sleep_diagnosis1' key exists in any dictionary in global_records\n",
    "sleep_diagnosis1_exists = any('sleep_diagnosis1' in record for record in global_records)\n",
    "print(f\"Does 'sleep_diagnosis1' key exist in any record? {sleep_diagnosis1_exists}\")\n",
    "\n",
    "# Display records that have the 'sleep_diagnosis1' key (if any)\n",
    "records_with_sleep_diagnosis1 = [record for record in global_records if 'sleep_diagnosis1' in record]\n",
    "if records_with_sleep_diagnosis1:\n",
    "    print(f\"\\nFound {len(records_with_sleep_diagnosis1)} records with 'sleep_diagnosis1' key:\")\n",
    "    for i, record in enumerate(records_with_sleep_diagnosis1[:3]):  # Show first 3 examples\n",
    "        print(f\"Record {i+1}: {record}\")\n",
    "    if len(records_with_sleep_diagnosis1) > 3:\n",
    "        print(f\"... and {len(records_with_sleep_diagnosis1) - 3} more records\")\n",
    "else:\n",
    "    print(\"No records contain the 'sleep_diagnosis1' key\")\n",
    "\n",
    "# Still display the global_records for reference\n",
    "global_records"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Find healthCodes where there are no labels at all\n",
    "# First identify all label columns (those ending with '_value')\n",
    "label_columns = [col for col in df1.columns if col.endswith('_value')]\n",
    "\n",
    "# For each healthCode, check if all label columns are NaN\n",
    "mask = df1[label_columns].isna().all(axis=1)\n",
    "healthcodes_without_labels = df1.loc[mask, 'healthCode'].unique()\n",
    "\n",
    "print(f\"Found {len(healthcodes_without_labels)} healthCodes with no labels:\")\n",
    "print(healthcodes_without_labels[:10])  # Show first 10 as example\n",
    "if len(healthcodes_without_labels) > 10:\n",
    "    print(f\"... and {len(healthcodes_without_labels) - 10} more\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_dataset import FilteredMhcDataset\n",
    "\n",
    "happiness_dataset = FilteredMhcDataset(\n",
    "    dataframe=df1, \n",
    "    root_dir=\"blabla\", \n",
    "    label_of_interest='happiness'\n",
    ")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
