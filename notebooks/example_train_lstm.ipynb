{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import os\n",
    "import matplotlib.pyplot as plt\n",
    "from torch.utils.data import DataLoader, random_split\n",
    "\n",
    "# Import the MHC dataset and LSTM model\n",
    "from torch_dataset import BaseMhcDataset\n",
    "from models.lstm import AutoencoderLSTM, LSTMTrainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "standardization_df = pd.read_csv(\"/scratch/users/schuetzn/data/mhc_dataset_out/standardization_params.csv\")\n",
    "\n",
    "scaler_stats = {}\n",
    "for f_idx, row in standardization_df.iloc[:6].iterrows():\n",
    "    scaler_stats[f_idx] = (row[\"mean\"], row[\"std_dev\"])\n",
    "\n",
    "scaler_stats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Set random seed for reproducibility\n",
    "torch.manual_seed(42)\n",
    "np.random.seed(42)\n",
    "\n",
    "# Get input from user\n",
    "dataset_parquet_path = \"/scratch/users/schuetzn/data/mhc_dataset_out/splits/train_dataset.parquet\"\n",
    "root_dir = \"/scratch/groups/euan/mhc/mhc_dataset\"\n",
    "\n",
    "print(f\"Loading dataset from {dataset_parquet_path}\")\n",
    "print(f\"Using root directory: {root_dir}\")\n",
    "\n",
    "# Load the denormalized dataset from parquet\n",
    "df = pd.read_parquet(dataset_parquet_path)\n",
    "df[\"file_uris\"] = df[\"file_uris\"].apply(eval)\n",
    "\n",
    "print(f\"Loaded dataset with {len(df)} samples\")\n",
    "\n",
    "# Print available label columns\n",
    "label_cols = [col for col in df.columns if col.endswith('_value')]\n",
    "print(f\"Available label columns: {label_cols}\")\n",
    "\n",
    "# Create the dataset with mask\n",
    "dataset = BaseMhcDataset(df, root_dir, include_mask=True, feature_stats=scaler_stats, feature_indices=list(range(6)))\n",
    "\n",
    "# Split into train and validation sets (80% train, 20% validation)\n",
    "train_size = int(0.8 * len(dataset))\n",
    "val_size = len(dataset) - train_size\n",
    "train_dataset, val_dataset = random_split(dataset, [train_size, val_size])\n",
    "print(f\"Split dataset into {train_size} training and {val_size} validation samples\")\n",
    "\n",
    "# Create data loaders\n",
    "batch_size = 128\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True, num_workers=16)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, num_workers=16)\n",
    "\n",
    "# Initialize model\n",
    "# Set target_labels to None if there are no labels you want to predict\n",
    "# Or specify the labels you want to predict (without '_value' suffix)\n",
    "target_labels = [] #[label.replace('_value', '') for label in label_cols[:2]] if label_cols else None\n",
    "\n",
    "model = AutoencoderLSTM(\n",
    "    num_features=6,\n",
    "    hidden_size=256,\n",
    "    encoding_dim=256,\n",
    "    num_layers=5,\n",
    "    dropout=0.1,\n",
    "    bidirectional=False,\n",
    "    target_labels=target_labels,\n",
    "    prediction_horizon=1,\n",
    "    use_masked_loss=True,\n",
    "    teacher_forcing_ratio=1\n",
    ")\n",
    "print(f\"Initialized LSTM model with target labels: {target_labels}\")\n",
    "# Calculate total parameters\n",
    "total_params = sum(p.numel() for p in model.parameters())\n",
    "\n",
    "# Calculate trainable parameters (parameters that require gradients)\n",
    "trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n",
    "\n",
    "print(f\"Total parameters: {total_params:,}\")\n",
    "print(f\"Trainable parameters: {trainable_params:,}\") \n",
    "\n",
    "# Set up optimizer\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "\n",
    "# Set up trainer (no changes needed here)\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "print(f\"Using device: {device}\")\n",
    "trainer = LSTMTrainer(model, optimizer, device)\n",
    "\n",
    "# --- Teacher Forcing Decay Parameters ---\n",
    "initial_tf = 1  # Start with 80% teacher forcing\n",
    "final_tf = 0.0    # Decay to 0%\n",
    "decay_epochs = 15 # Decay over 15 epochs (e.g., first 15 epochs)\n",
    "tf_decay_step = (initial_tf - final_tf) / max(1, decay_epochs) # Avoid division by zero\n",
    "\n",
    "# Train the model\n",
    "num_epochs = 20 # Example: Train for 20 epochs total\n",
    "print(f\"Training for {num_epochs} epochs...\")\n",
    "print(f\"Teacher forcing decay: {initial_tf*100:.0f}% -> {final_tf*100:.0f}% over {decay_epochs} epochs.\")\n",
    "\n",
    "train_losses = []\n",
    "val_losses = []\n",
    "\n",
    "for epoch in range(num_epochs): # epoch will be 0, 1, 2, ...\n",
    "\n",
    "    # --- Calculate and set teacher forcing ratio for this epoch ---\n",
    "    if epoch < decay_epochs:\n",
    "        current_tf_ratio = initial_tf - tf_decay_step * epoch\n",
    "        # Ensure it doesn't go below the final ratio\n",
    "        trainer.model.teacher_forcing_ratio = max(final_tf, current_tf_ratio) \n",
    "    else:\n",
    "        # After decay period, keep it at the final ratio\n",
    "        trainer.model.teacher_forcing_ratio = final_tf\n",
    "        \n",
    "    current_tf_ratio_for_print = trainer.model.teacher_forcing_ratio # Get the value actually set\n",
    "\n",
    "    # --- Train for one epoch (no need to pass epoch here) ---\n",
    "    train_loss = trainer.train_epoch(train_loader) \n",
    "    \n",
    "    # --- Validate (teacher forcing is off in eval mode anyway) ---\n",
    "    val_loss = trainer.validate(val_loader)\n",
    "    \n",
    "    train_losses.append(train_loss)\n",
    "    val_losses.append(val_loss)\n",
    "    \n",
    "    print(f\"Epoch {epoch+1}/{num_epochs}, Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n",
    "\n",
    "# Plot training and validation loss\n",
    "plt.figure(figsize=(10, 5))\n",
    "plt.plot(train_losses, label='Training Loss')\n",
    "plt.plot(val_losses, label='Validation Loss')\n",
    "plt.xlabel('Epoch')\n",
    "plt.ylabel('Loss')\n",
    "plt.title('Training and Validation Loss')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.show()\n",
    "\n",
    "# Make predictions on a sample\n",
    "print(\"Making predictions on a validation sample...\")\n",
    "with torch.no_grad():\n",
    "    # Get a sample from validation set\n",
    "    sample_idx = 0\n",
    "    sample = val_dataset[sample_idx]\n",
    "    \n",
    "    # Print metadata for the sample\n",
    "    print(f\"Sample metadata: {sample['metadata']}\")\n",
    "    \n",
    "    # Prepare batch format\n",
    "    batch = {\n",
    "        'data': sample['data'].unsqueeze(0).to(device),  # Add batch dimension\n",
    "        'mask': sample['mask'].unsqueeze(0).to(device)\n",
    "    }\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    output = model(batch)\n",
    "    \n",
    "    # Get predictions\n",
    "    predicted_segments = output['sequence_output'][0].cpu().numpy()  # Remove batch dimension\n",
    "    target_segments = output['target_segments'][0].cpu().numpy()\n",
    "    \n",
    "    # Visualize prediction for a single feature and segment\n",
    "    feature_idx = 0  # First feature\n",
    "    segment_idx = 0  # First segment\n",
    "    \n",
    "    # Extract the first 30 values\n",
    "    predicted_values = predicted_segments[segment_idx, :30]\n",
    "    target_values = target_segments[segment_idx, :30]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(predicted_values, label='Predicted', marker='o')\n",
    "    plt.plot(target_values, label='Target', marker='x')\n",
    "    plt.xlabel('Time index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Prediction vs Target for Feature {feature_idx}, Segment {segment_idx}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "model, trainer  # Return model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Make predictions on a sample\n",
    "print(\"Making predictions on a validation sample...\")\n",
    "with torch.no_grad():\n",
    "    # Get a sample from validation set\n",
    "    sample_idx = 1000\n",
    "    sample = val_dataset[sample_idx]\n",
    "    \n",
    "    # Print metadata for the sample\n",
    "    print(f\"Sample metadata: {sample['metadata']}\")\n",
    "    \n",
    "    # Prepare batch format\n",
    "    batch = {\n",
    "        'data': sample['data'].unsqueeze(0).to(device),  # Add batch dimension\n",
    "        'mask': sample['mask'].unsqueeze(0).to(device)\n",
    "    }\n",
    "    \n",
    "    # Forward pass\n",
    "    model.eval()\n",
    "    output = model(batch)\n",
    "    \n",
    "    # Get predictions\n",
    "    predicted_segments = output['sequence_output'][0].cpu().numpy()  # Remove batch dimension\n",
    "    target_segments = output['target_segments'][0].cpu().numpy()\n",
    "    \n",
    "    # Visualize prediction for a single feature and segment\n",
    "    feature_idx = 3  # First feature\n",
    "    segment_idx = 116  # First segment\n",
    "    \n",
    "    # Extract the first 30 values\n",
    "    predicted_values = predicted_segments[segment_idx, :30]\n",
    "    target_values = target_segments[segment_idx, :30]\n",
    "    \n",
    "    plt.figure(figsize=(12, 4))\n",
    "    plt.plot(predicted_values, label='Predicted', marker='o')\n",
    "    plt.plot(target_values, label='Target', marker='x')\n",
    "    plt.xlabel('Time index')\n",
    "    plt.ylabel('Value')\n",
    "    plt.title(f'Prediction vs Target for Feature {feature_idx}, Segment {segment_idx}')\n",
    "    plt.legend()\n",
    "    plt.grid(True)\n",
    "    plt.show()\n",
    "\n",
    "model, trainer  # Return model and trainer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from torch.utils.data import Dataset, Subset\n",
    "\n",
    "\n",
    "def visualize_sample_day_feature(\n",
    "    dataset, \n",
    "    sample_idx: int, \n",
    "    day_index: int, \n",
    "    feature_index: int,\n",
    "    include_mask_overlay: bool = True\n",
    "):\n",
    "    \"\"\"\n",
    "    Visualizes the time series data for a specific feature and day within a dataset sample.\n",
    "\n",
    "    Args:\n",
    "        dataset: An initialized instance of BaseMhcDataset or a Subset wrapping it.\n",
    "        sample_idx: The index of the sample to visualize (relative to the dataset/subset).\n",
    "        day_index: The index of the day within the sample's time range (0-based).\n",
    "        feature_index: The index of the feature channel to visualize (0-based, typically 0-23).\n",
    "        include_mask_overlay: If True and the dataset includes masks, highlights the \n",
    "                              masked/missing time points on the plot.\n",
    "    \"\"\"\n",
    "    \n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(dataset, (Dataset, Subset)):\n",
    "         print(\"Error: 'dataset' must be a PyTorch Dataset or Subset.\")\n",
    "         return\n",
    "        \n",
    "    if sample_idx < 0 or sample_idx >= len(dataset):\n",
    "        print(f\"Error: sample_idx {sample_idx} is out of bounds for dataset/subset of size {len(dataset)}.\")\n",
    "        return\n",
    "\n",
    "    # --- Determine the underlying dataset and its properties ---\n",
    "    # Access the original dataset, whether it's the input or wrapped in a Subset\n",
    "    original_dataset = dataset.dataset if isinstance(dataset, Subset) else dataset\n",
    "    # Check include_mask attribute on the original dataset\n",
    "    dataset_includes_mask = getattr(original_dataset, 'include_mask', False)\n",
    "\n",
    "    # --- Retrieve Sample ---\n",
    "    try:\n",
    "        # Getting the item works the same for Dataset and Subset\n",
    "        sample = dataset[sample_idx] \n",
    "        data_tensor = sample['data'] # Shape: (num_days, 24, 1440)\n",
    "        metadata = sample.get('metadata', {})\n",
    "        health_code = metadata.get('healthCode', 'N/A')\n",
    "        time_range = metadata.get('time_range', 'N/A')\n",
    "        \n",
    "        # Check if the *sample* contains a mask AND the *original dataset* was set to include them\n",
    "        has_mask = 'mask' in sample and dataset_includes_mask\n",
    "        mask_tensor = sample['mask'] if has_mask else None # Shape: (num_days, 24, 1440) or None\n",
    "        \n",
    "    except IndexError:\n",
    "        print(f\"Error: Could not retrieve sample at index {sample_idx}.\")\n",
    "        return\n",
    "    except Exception as e:\n",
    "        print(f\"An error occurred while retrieving sample {sample_idx}: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Data Shape and Index Validation ---\n",
    "    num_days, num_features, num_minutes = data_tensor.shape\n",
    "    \n",
    "    if day_index < 0 or day_index >= num_days:\n",
    "        print(f\"Error: day_index {day_index} is out of bounds. Sample has {num_days} days.\")\n",
    "        return\n",
    "        \n",
    "    if feature_index < 0 or feature_index >= num_features:\n",
    "        print(f\"Error: feature_index {feature_index} is out of bounds. Sample has {num_features} features.\")\n",
    "        return\n",
    "        \n",
    "    if num_minutes != 1440:\n",
    "         print(f\"Warning: Expected 1440 minutes (time points), but found {num_minutes}.\")\n",
    "\n",
    "    # --- Extract Data and Mask for Plotting ---\n",
    "    feature_data = data_tensor[day_index, feature_index, :].cpu().numpy()\n",
    "    time_axis = np.arange(num_minutes)\n",
    "    \n",
    "    masked_indices = None\n",
    "    if include_mask_overlay and has_mask and mask_tensor is not None:\n",
    "        feature_mask = mask_tensor[day_index, feature_index, :].cpu().numpy()\n",
    "        # Mask == 0 means the data point is missing/masked\n",
    "        masked_indices = np.where(feature_mask == 0)[0] \n",
    "        print(f\"Found {len(masked_indices)} masked points for this feature/day.\")\n",
    "\n",
    "\n",
    "    # --- Plotting ---\n",
    "    plt.figure(figsize=(15, 5))\n",
    "    \n",
    "    # Plot the main data\n",
    "    plt.plot(time_axis, feature_data, label=f'Feature {feature_index} Data', color='dodgerblue', linewidth=1)\n",
    "    \n",
    "    # Overlay masked points if requested and available\n",
    "    if include_mask_overlay and masked_indices is not None and len(masked_indices) > 0:\n",
    "        plt.scatter(time_axis[masked_indices], feature_data[masked_indices], \n",
    "                    color='red', marker='x', s=20, label='Masked/Missing (Mask=0)', zorder=5)\n",
    "\n",
    "    plt.title(f'Sample {sample_idx} (HC: {health_code}) - Day {day_index} - Feature {feature_index}\\nTime Range: {time_range}')\n",
    "    plt.xlabel('Minute of Day')\n",
    "    plt.ylabel('Feature Value (potentially standardized)')\n",
    "    plt.grid(True, linestyle='--', alpha=0.6)\n",
    "    plt.legend()\n",
    "    plt.xlim(0, num_minutes - 1)\n",
    "    plt.tight_layout()\n",
    "    plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "visualize_sample_day_feature(val_dataset, sample_idx=1005, day_index=3, feature_index=5, include_mask_overlay=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
